{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c607bd1-8fc4-40fb-aaa4-61c87b2df9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37b5cfc-5bcc-49fe-897c-94f3c5406481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chat_history(conversation_history: list) -> str:\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    conversation_text = \" \".join(conversation_history)\n",
    "    summary_result = summarizer(conversation_text, max_length=150, min_length=30, do_sample=False)\n",
    "    return summary_result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c1bec5d-c9d1-41ba-ad38-29f0baffdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(conversation_history: list, window_size: int = 5) -> list:\n",
    "    return conversation_history[-window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0295f91d-08a1-46ba-afc7-fe3e0d94bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_token_budgeting(conversation_history: list, max_tokens: int) -> list:\n",
    "    token_count = 0\n",
    "    selected_messages = []\n",
    "    for message in reversed(conversation_history):\n",
    "        message_tokens = len(message.split())\n",
    "        if token_count + message_tokens <= max_tokens:\n",
    "            token_count += message_tokens\n",
    "            selected_messages.insert(0, message)\n",
    "        else:\n",
    "            break\n",
    "    return selected_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942b79dd-a5ba-4f5a-bfd5-8dc8cca2f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_memory(conversation_history: list, short_term_window: int = 5) -> dict:\n",
    "    if len(conversation_history) <= short_term_window:\n",
    "        return {\"short_term\": conversation_history, \"long_term\": \"\"}\n",
    "    \n",
    "    short_term = conversation_history[-short_term_window:]\n",
    "    long_term_history = conversation_history[:-short_term_window]\n",
    "    long_term_summary = summarize_chat_history(long_term_history)\n",
    "    \n",
    "    return {\"short_term\": short_term, \"long_term\": long_term_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0116c996-0168-465a-8e4a-ae5089faca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_by_keyword(conversation_history: list, keyword: str) -> list:\n",
    "    return [msg for msg in conversation_history if keyword.lower() in msg.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb8d3d8-7f8e-4b34-a249-17fc72505a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 133. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Conversation:\n",
      " Bot: I'm sorry to hear that, can you provide your order number? User: Sure, it's 123456. Bot: Thank you, let me look into this . User: I also have a question about your return policy . Bot: Our return policy lasts 30 days .\n",
      "\n",
      "\n",
      "Sliding Window (Last 5 Messages):\n",
      "Bot: Our return policy lasts 30 days. Please let me know if you have any other questions or need further assistance.\n",
      "User: That helps a lot, thanks!\n",
      "Bot: You're welcome! Have a great day.\n",
      "User: I wonder if I can track my order online?\n",
      "Bot: Yes, you can track your order by logging into your account.\n",
      "\n",
      "\n",
      "Dynamic Token Budgeting (Max 30 Tokens):\n",
      "Bot: You're welcome! Have a great day.\n",
      "User: I wonder if I can track my order online?\n",
      "Bot: Yes, you can track your order by logging into your account.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Memory:\n",
      "Short Term:\n",
      "Bot: Our return policy lasts 30 days. Please let me know if you have any other questions or need further assistance.\n",
      "User: That helps a lot, thanks!\n",
      "Bot: You're welcome! Have a great day.\n",
      "User: I wonder if I can track my order online?\n",
      "Bot: Yes, you can track your order by logging into your account.\n",
      "Long Term Summary:\n",
      " Bot: I'm sorry to hear that, can you provide your order number? User: Sure, it's 123456. Bot: Thank you, let me look into this . User: I also have a question about your return policy .\n",
      "\n",
      "\n",
      "Retrieval by Keyword ('order'):\n",
      "User: Hi, I have a problem with my order.\n",
      "Bot: I'm sorry to hear that, can you provide your order number?\n",
      "User: I wonder if I can track my order online?\n",
      "Bot: Yes, you can track your order by logging into your account.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    conversation = [\n",
    "        \"User: Hi, I have a problem with my order.\",\n",
    "        \"Bot: I'm sorry to hear that, can you provide your order number?\",\n",
    "        \"User: Sure, it's 123456.\",\n",
    "        \"Bot: Thank you, let me look into this.\",\n",
    "        \"User: I also have a question about your return policy.\",\n",
    "        \"Bot: Our return policy lasts 30 days. Please let me know if you have any other questions or need further assistance.\",\n",
    "        \"User: That helps a lot, thanks!\",\n",
    "        \"Bot: You're welcome! Have a great day.\",\n",
    "        \"User: I wonder if I can track my order online?\",\n",
    "        \"Bot: Yes, you can track your order by logging into your account.\"\n",
    "    ]\n",
    "    summary = summarize_chat_history(conversation)\n",
    "    print(\"Summarized Conversation:\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")\n",
    "    sliding = sliding_window(conversation, window_size=5)\n",
    "    print(\"Sliding Window (Last 5 Messages):\")\n",
    "    for msg in sliding:\n",
    "        print(msg)\n",
    "    print(\"\\n\")\n",
    "    dynamic_history = dynamic_token_budgeting(conversation, max_tokens=30)\n",
    "    print(\"Dynamic Token Budgeting (Max 30 Tokens):\")\n",
    "    for msg in dynamic_history:\n",
    "        print(msg)\n",
    "    print(\"\\n\")\n",
    "    memory = hierarchical_memory(conversation, short_term_window=5)\n",
    "    print(\"Hierarchical Memory:\")\n",
    "    print(\"Short Term:\")\n",
    "    for msg in memory['short_term']:\n",
    "        print(msg)\n",
    "    print(\"Long Term Summary:\")\n",
    "    print(memory['long_term'])\n",
    "    print(\"\\n\")\n",
    "    keyword_results = retrieval_by_keyword(conversation, keyword=\"order\")\n",
    "    print(\"Retrieval by Keyword ('order'):\")\n",
    "    for msg in keyword_results:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236e16e-60ea-4d97-8954-5580a07c0b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
